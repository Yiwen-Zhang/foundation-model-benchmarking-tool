# This file contains the evaluation information for majority voting. Here, we initialize
# the embeddings model used to calculate quantitative metrics such as 
# cosine similarity. The other part of this evaluation is using subjective
# evaluation methods: majority voting. In the case of when a ground truth
# is provided, FMBench can use majority voting with the help of a 'panel of judges' to get a verdict [correct, incorrect].
# For more information, view this paper: https://arxiv.org/pdf/2404.18796. Majority voting using a panel of LLM evaluators
# helps in getting a 'close to human evaluation', reduces cost of evaluations, and eliminates intra model bias.
model_evaluations:
  ground_truth_col: {ground_truth}

  PoLL_Composition_and_Voting: 
    method: majority_vote
    # Set this variable to yes if you want to make partial correct/incorrect decisions based
    # on quantitative metrics like cosine similarity, levenshtein score and token set ratio. Set
    # this to yes only if you have a very direct QnA use case
    use_quantitative_metrics: no

  # This represents the information that is used to get the quantitative metrics 
  # from the evaluation step. This includes calculating the cosine similarity. 
  # If a ground truth is provided, measure the cosine similarity against the ground truth, 
  # else measure it against the context provided. We use the `sentence-transformers/all-mpnet-base-v2`
  # dataset. There is also an option to use the Titan embeddings model (WIP)
  quantitative_eval_info:
    embeddings_model_id:
      model_id: sentence-transformers/all-mpnet-base-v2
    # This contains information about quantitative metrics thresholds that need to be set while
    # evaluating whether a candidate model response is correct or incorrect without parsing it through
    # the panel of LLM evaluation procedure
    # this is the budget for the answer to be correct from a candidate model
    # if the cosine similarity for the model response is beyond 0.85, we consider 
    # 'cosine_similarity_threshold' correct
    cosine_similarity_threshold: 0.95
    # this is the second criteria. This depends on the token set ratio
    token_set_ratio_threshold: 1.00
    # this is the third criteria for the check against the levenshtein distance
    levenshtein_distance_threshold: 0.90
    # If none of the eval criteria above are met, then check for 
    # of the average of them all are above this threshold.
    overall_eval_threshold: 0.90
  # This represents the information that is used to get subjective evaluations on the 
  # content that is generated. It uses an LLM as a judge (that is configurable) and evaluates
  # each content from the inference step on different evaluation criteria. The information about 
  # the LLM as a judge panel is given below that is used in the majority voting
  subjective_eval_info:
    # this is the judge panel list that is used in the evaluation process
    judge_panel_list:
      # Information on judge 1 on the evaluation judge panel
      - model_id: meta.llama3-70b-instruct-v1:0
        # this is the prompt template that is used in the evaluation process
        # based on the method: majority voting
        eval_prompt_template_dir: "llama3_eval_prompt_templates"
        eval_prompt_template_name: "llama3_eval_{method_name}"
      # Information on judge 2 on the evaluation judge panel
      - model_id: anthropic.claude-3-sonnet-20240229-v1:0
        # this is the prompt template that is used in the evaluation process
        # based on the method: majority voting
        eval_prompt_template_dir: "claude_eval_prompt_templates"
        eval_prompt_template_name: "claude_eval_{method_name}"
      # Information on judge 3 on the evaluation judge panel
      # We use the most powerful cohere model - cohere command R +
      - model_id: cohere.command-r-plus-v1:0
        # this is the prompt template that is used in the evaluation process
        # based on the method: majority voting
        eval_prompt_template_dir: "cohere_eval_prompt_templates"
        eval_prompt_template_name: "cohere_eval_{method_name}"
    # number of parallel calls made asyncronously to bedrock using Ray
    run_parallel_inference_count: 15
    # Common inference parameters used in the evaluation process
    # We use LiteLLM for interfacing with Bedrock
    inference_parameters:
      temperature: 0.1
      max_tokens: 300
      top_p: 0.92
      caching: False
    
