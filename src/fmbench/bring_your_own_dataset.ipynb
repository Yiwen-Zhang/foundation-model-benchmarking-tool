{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d144a1ec-e66d-45b8-baf1-af4641ee23ce",
   "metadata": {},
   "source": [
    "# Bring your own dataset\n",
    "\n",
    "---------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "### This part of our solution design includes \n",
    "\n",
    "- Creating your own `fmbench` compatible dataset from a [HuggingFace dataset](https://huggingface.co/docs/datasets/en/index).\n",
    "\n",
    "- Creating a prompt payload template compatible with your dataset.\n",
    "\n",
    "- Upload the dataset and the prompt payload to Amazon S3 from where it can be used by `fmbench`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eafa135b-2dec-4e2c-9821-bd8268a21492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7343d0fb-89cc-48da-8c77-22b8024a7e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_name=us-east-1\n",
      "role_arn_from_env=None, using current sts caller identity to set arn_string\n",
      "the sts role is an assumed role, setting arn_string to arn:aws:iam::471112568442:role/fmbench-us-east-1-role\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from datasets import load_dataset\n",
    "config = load_config(CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89998f93-e59a-4d47-9957-ad0b11a089f7",
   "metadata": {},
   "source": [
    "## Convert HuggingFace dataset to jsonl format\n",
    "\n",
    "`fmbench` works with datasets in the [`JSON Lines`](https://jsonlines.org/) format. So here we show how to convert a HuggingFace dataset into JSON lines format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d021d-ff1c-4275-85ae-128265b0d5fa",
   "metadata": {},
   "source": [
    "Set the `ds_name` to the HuggingFace dataset id, for example [`THUDM/LongBench`](https://huggingface.co/datasets/THUDM/LongBench), [`rajpurkar/squad_v2`](https://huggingface.co/datasets/rajpurkar/squad_v2), [`banking77`](https://huggingface.co/datasets/banking77) or other text datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "657bad51-f5dd-45e0-a08c-cbfd33206301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ds_id: str = \"rajpurkar/squad\"\n",
    "# ds_name: str = \"plain_text\"\n",
    "# ds_split: str = \"train\"\n",
    "# # Take a random subset of the dataframe, adjust the value of `N` below as appropriate.\n",
    "# # size of random subset of the data\n",
    "# ds_N: int = 100\n",
    "\n",
    "# another example\n",
    "# ds_id: str = \"THUDM/LongBench\"\n",
    "# ds_name: str = \"2wikimqa\"\n",
    "# ds_split: str = \"test\"\n",
    "# Take a random subset of the dataframe, adjust the value of `N` below as appropriate.\n",
    "# size of random subset of the data\n",
    "# ds_N: int = 200\n",
    "\n",
    "# another example\n",
    "ds_id: str = \"banking77\"\n",
    "ds_name: str = \"default\"\n",
    "ds_split: str = \"train\"\n",
    "# Take a random subset of the dataframe, adjust the value of `N` below as appropriate.\n",
    "# size of random subset of the data\n",
    "ds_N: int = 100\n",
    "\n",
    "# ds_id: str = \"Open-Orca/OpenOrca\"\n",
    "# ds_name: str = \"default\"\n",
    "# ds_split: str = \"train\"\n",
    "# # Take a random subset of the dataframe, adjust the value of `N` below as appropriate.\n",
    "# # size of random subset of the data\n",
    "# ds_N: int = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "707c44bd-af91-4268-9d76-35544b0653e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cba4adcc49e493f9403e4cc345d487f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/14.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8023bc9637174a03b53447669ac52c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/298k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81f304c302743bc9ff3f0c367ee16d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/93.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1100384db6e47e78887181796bbbcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545a153bfaee4e02982b2a8e661d9969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset from huggingface\n",
    "dataset = load_dataset(ds_id, name=ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e558518a-c24e-4f6d-becf-3fdbf6c10e78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10003\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3080\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9c55120-b66a-4386-8371-d5544e3d54d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataset to a dataframe, for print it out and easy conversion to jsonl\n",
    "df = pd.DataFrame(dataset[ds_split])\n",
    "\n",
    "# some datasets contain a field called column, we would like to call it\n",
    "# input to match it to the prompt template\n",
    "df.rename(columns={\"question\": \"input\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff18dd26-ad8f-449d-bcba-1dd96f00b6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am still waiting on my card?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What can I do if my card still hasn't arrived ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have been waiting over a week. Is the card s...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can I track my card while it is in the process...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I know if I will get my card, or if it ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                     I am still waiting on my card?     11\n",
       "1  What can I do if my card still hasn't arrived ...     11\n",
       "2  I have been waiting over a week. Is the card s...     11\n",
       "3  Can I track my card while it is in the process...     11\n",
       "4  How do I know if I will get my card, or if it ...     11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703cc0f9-a86b-42de-81b4-21a6a3048fe7",
   "metadata": {},
   "source": [
    "Subset the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30c4e481-2cd4-4341-b910-c5172eafe400",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape before random subset = (10003, 2)\n",
      "dataset shape before random subset = (100, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset shape before random subset = {df.shape}\")\n",
    "df = df.sample(n=ds_N)\n",
    "print(f\"dataset shape before random subset = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fcadf-0ca7-4123-92d6-ed7b188cfd4b",
   "metadata": {},
   "source": [
    "Convert to json lines format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adb38f44-9788-4d26-b77e-2dde3feddfb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\":\"I'm still expecting the transaction to be finished\",\"label\":66}\n",
      "{\"text\":\"What are the limits to where my card will be accepted?\",\"label\":10}\n",
      "{\"text\":\"I am not sure where my phone is.\",\"label\":42}\n",
      "{\"text\":\"Still waiting on my refund...\",\"label\":51}\n",
      "{\"text\":\"I'm not satisfied with the services that you are providing.  I would like to end my services and delete my account.\",\"label\":55}\n",
      "{\"text\":\"Should I be seeing a fee applied for my money transfer?\",\"label\":64}\n",
      "{\"text\":\"Why did my card payment not work?\",\"label\":25}\n",
      "{\"text\":\"Can I choose when my card is delivered?\",\"label\":12}\n",
      "{\"text\":\"Would I be charged any fees if I added money to my account using an international card?\",\"label\":57}\n",
      "{\"text\":\"There is a payment made with my card that I don't recognize at all.\",\"label\":16}\n",
      "{\"text\":\"How soon do cards arrive after I order them?\",\"label\":12}\n",
      "{\"text\":\"Why did the ATM swallow my card?\",\"label\":18}\n",
      "{\"text\":\"Hey, I have my card, how do I get it to show in the app?\",\"label\":13}\n",
      "{\"text\":\n"
     ]
    }
   ],
   "source": [
    "jsonl_content = df.to_json(orient='records', lines=True)\n",
    "print(jsonl_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c64b4-d0c8-41d5-80a2-86581d176a20",
   "metadata": {},
   "source": [
    "## Upload the dataset to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bfecbb1-6355-4f52-86a4-53579d867629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-fmbench-read-us-east-1-471112568442/source_data/banking77.jsonl'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket: str = config['s3_read_data']['read_bucket']\n",
    "prefix: str = config['s3_read_data']['source_data_prefix']\n",
    "file_name: str = f\"{ds_id}.jsonl\"\n",
    "json(jsonl_content, bucket, prefix, \"\", file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6fd45-fc80-45f3-a184-9cad6a1cd706",
   "metadata": {},
   "source": [
    "## Create a prompt template and upload it to S3\n",
    "The prompt template is specific to the model under test and also the dataset being used. The variables used in the template, such as `context` and `input` must exist in the dataset being used so that this prompt template can be converted into an actual prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "018df5ca-dd8a-4436-a1a6-c3204ac079d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dictionary containing the prompt template, it has a key by the name\n",
    "# of the dataset id which forces you to explicitly add your dataset here\n",
    "# otherwise no new prompt template will be uploaded and it wont accidently\n",
    "# end up overwriting an existing prompt template\n",
    "prompt_template = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ef34360-c855-49e6-a624-0209ba09939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LongBench\n",
    "prompt_template['THUDM-LongBench-llama2-mistral'] = \"\"\"<s>[INST] <<SYS>>\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "<</SYS>>\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "[/INST]\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac185446-2338-47ec-8c78-30dea736aaec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open Orca\n",
    "prompt_template['Open-Orca-OpenOrca-llama2-mistral'] = \"\"\"<s>[INST] <<SYS>>\n",
    "\n",
    "{system_prompt}\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "Context and task: {input}\n",
    "\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12d0009c-79b8-4cf9-8450-824a03ae5996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template['Open-Orca-OpenOrca-llama3'] = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{system_prompt}\n",
    "\n",
    "Context and task: {input} \n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5771c28e-07c6-4805-b33d-186895496aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing prompt_template_THUDM-LongBench-llama2-mistral.txt to s3://sagemaker-fmbench-read-us-east-1-471112568442/prompt_template/prompt_template_THUDM-LongBench-llama2-mistral.txt\n",
      "writing prompt_template_Open-Orca-OpenOrca-llama2-mistral.txt to s3://sagemaker-fmbench-read-us-east-1-471112568442/prompt_template/prompt_template_Open-Orca-OpenOrca-llama2-mistral.txt\n",
      "writing prompt_template_Open-Orca-OpenOrca-llama3.txt to s3://sagemaker-fmbench-read-us-east-1-471112568442/prompt_template/prompt_template_Open-Orca-OpenOrca-llama3.txt\n"
     ]
    }
   ],
   "source": [
    "bucket: str = config['s3_read_data']['read_bucket']\n",
    "prefix: str = config['s3_read_data']['prompt_template_dir']\n",
    "for k in prompt_template.keys():\n",
    "    file_name: str = f\"prompt_template_{k}.txt\"\n",
    "    print(f\"writing {file_name} to s3://{bucket}/{prefix}/{file_name}\")\n",
    "    write_to_s3(prompt_template[k], bucket, prefix, \"\", file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25ebd9-782a-4765-b612-2731468c5d63",
   "metadata": {},
   "source": [
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d11fdc-ac92-42a7-b9de-8a5d36050532",
   "metadata": {},
   "source": [
    "### Utility function for converting a line from container log to JSON format\n",
    "\n",
    "The following is a line from CW log from a model container that provides all the information about the model that is not available anywhere else (not in Model or EndpointConfig or Endpoint description). This information is often necessary to know the low level settings about the model which may have been set while compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f71becec-c2c7-4145-ad9d-84ab16e79aff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n",
      "rolling_batch=<RollingBatchEnum.auto: 'auto'>\n",
      "tensor_parallel_degree=8\n",
      "trust_remote_code=False\n",
      "enable_streaming=<StreamingEnum.false: 'false'>\n",
      "batch_size=4\n",
      "max_rolling_batch_size=4\n",
      "dtype=<Dtype.f16: 'fp16'>\n",
      "revision=None\n",
      "output_formatter=None\n",
      "waiting_steps=None\n",
      "is_mpi=False\n",
      "draft_model_id=None\n",
      "spec_length=0\n",
      "neuron_optimize_level=None\n",
      "enable_mixed_precision_accumulation=False\n",
      "enable_saturate_infinity=False\n",
      "n_positions=4096\n",
      "unroll=None\n",
      "load_in_8bit=False\n",
      "low_cpu_mem_usage=False\n",
      "load_split_model=True\n",
      "context_length_estimate=None\n",
      "amp='f16'\n",
      "quantize=None\n",
      "compiled_graph_path=None\n",
      "task=None\n",
      "save_mp_checkpoint_path=None\n",
      "group_query_attention=None\n",
      "model_loader=<TnXModelLoaders.tnx: 'tnx'>\n",
      "rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n",
      "fuse_qkv=False\n",
      "on_device_embedding=False\n",
      "attention_layout=None\n",
      "collectives_layout=None\n",
      "cache_layout=None\n",
      "partition_schema=None\n",
      "all_reduce_dtype=None\n",
      "cast_logits_dtype=None\n",
      "{\n",
      "  \"model_id_or_path\": \"'/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\",\n",
      "  \"rolling_batch\": \"<RollingBatchEnum.auto: 'auto'>\",\n",
      "  \"tensor_parallel_degree\": \"8\",\n",
      "  \"trust_remote_code\": \"False\",\n",
      "  \"enable_streaming\": \"<StreamingEnum.false: 'false'>\",\n",
      "  \"batch_size\": \"4\",\n",
      "  \"max_rolling_batch_size\": \"4\",\n",
      "  \"dtype\": \"<Dtype.f16: 'fp16'>\",\n",
      "  \"revision\": \"None\",\n",
      "  \"output_formatter\": \"None\",\n",
      "  \"waiting_steps\": \"None\",\n",
      "  \"is_mpi\": \"False\",\n",
      "  \"draft_model_id\": \"None\",\n",
      "  \"spec_length\": \"0\",\n",
      "  \"neuron_optimize_level\": \"None\",\n",
      "  \"enable_mixed_precision_accumulation\": \"False\",\n",
      "  \"enable_saturate_infinity\": \"False\",\n",
      "  \"n_positions\": \"4096\",\n",
      "  \"unroll\": \"None\",\n",
      "  \"load_in_8bit\": \"False\",\n",
      "  \"low_cpu_mem_usage\": \"False\",\n",
      "  \"load_split_model\": \"True\",\n",
      "  \"context_length_estimate\": \"None\",\n",
      "  \"amp\": \"'f16'\",\n",
      "  \"quantize\": \"None\",\n",
      "  \"compiled_graph_path\": \"None\",\n",
      "  \"task\": \"None\",\n",
      "  \"save_mp_checkpoint_path\": \"None\",\n",
      "  \"group_query_attention\": \"None\",\n",
      "  \"model_loader\": \"<TnXModelLoaders.tnx: 'tnx'>\",\n",
      "  \"rolling_batch_strategy\": \"<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\",\n",
      "  \"fuse_qkv\": \"False\",\n",
      "  \"on_device_embedding\": \"False\",\n",
      "  \"attention_layout\": \"None\",\n",
      "  \"collectives_layout\": \"None\",\n",
      "  \"cache_layout\": \"None\",\n",
      "  \"partition_schema\": \"None\",\n",
      "  \"all_reduce_dtype\": \"None\",\n",
      "  \"cast_logits_dtype\": \"None\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "line=\"\"\"model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408' rolling_batch=<RollingBatchEnum.auto: 'auto'> tensor_parallel_degree=8 trust_remote_code=False enable_streaming=<StreamingEnum.false: 'false'> batch_size=4 max_rolling_batch_size=4 dtype=<Dtype.f16: 'fp16'> revision=None output_formatter=None waiting_steps=None is_mpi=False draft_model_id=None spec_length=0 neuron_optimize_level=None enable_mixed_precision_accumulation=False enable_saturate_infinity=False n_positions=4096 unroll=None load_in_8bit=False low_cpu_mem_usage=False load_split_model=True context_length_estimate=None amp='f16' quantize=None compiled_graph_path=None task=None save_mp_checkpoint_path=None group_query_attention=None model_loader=<TnXModelLoaders.tnx: 'tnx'> rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'> fuse_qkv=False on_device_embedding=False attention_layout=None collectives_layout=None cache_layout=None partition_schema=None all_reduce_dtype=None cast_logits_dtype=None\"\"\"\n",
    "import re\n",
    "import json\n",
    "pattern = r' (?=[^\\'\"])'\n",
    "\n",
    "\n",
    "# Split the string using the pattern\n",
    "result = re.split(pattern, line)\n",
    "print(\"\\n\".join([r for r in result]))\n",
    "params= {}\n",
    "for kv in result:\n",
    "    #print(kv.split('='))\n",
    "    k,v = kv.split('=')\n",
    "    params[k] = v\n",
    "print(json.dumps(params, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c6e72-c7d7-46ae-aec9-67400130120e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
