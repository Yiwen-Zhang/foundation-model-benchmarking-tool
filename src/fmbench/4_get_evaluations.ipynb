{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Evaluations on inferences generated by candidate models, gather findings on quantitative metrics (such as _Cosine Similarity, levenshtein distance, and token set ratio_) and Subjective Metrics using Majority Voting with PoLL (Panel of LLM Evaluators)\n",
    "\n",
    "---------------------\n",
    "*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n",
    "\n",
    "#### This step of the solution gets evaluations on the quality of responses generated by the candidate models that have to be evaluated. It does so by performing the steps below:\n",
    "\n",
    "- **Gets the inference request file that contains all results from the inference stage**: The inference request results file containing all inferences from each candidate model is fetched along with the associated metrics such as ground truth (if any), source payload file, concurrency level, etc.\n",
    "\n",
    "- **Generates quantitative metrics for evaluation**: Calculate quantitative metrics to measure similarity and accuracy, using _Cosine Similarity, levenshtein distance, and token set ratio_. Cosine similarity is a metric used to measure how similar two vectors are, regardless of their size. Levenshtein distance is a string metric for measuring the difference between two sequences. The Token Set Ratio algorithm tokenizes both input strings, removes duplicate tokens, and calculates the similarity score. This helps in getting a quantitative overall score for the entire dataset in terms of which model generates outputs that are most similar and accurate to the ground truth provided. We use these metrics to build a hierarchy evaluation decision tree to move up to the next step of evaluation if the correctness of an answer is not obviously determined using the quantitative metrics only. \n",
    "    \n",
    "    The steps that are followed as a part of this evaluation hierarchy (for Majority Voting) is as given below:\n",
    "    \n",
    "    1. First check if either the _Cosine Similarity, levenshtein similarity, or token set ratio_ values or if the average of all three exceed a given threshold. If they do, users have the ability to make quantitative based decisions only. Which means if all quantitative metric values surpass the threshold, those answers can be evaluated as `correct` automatically without parsing them through an LLM evaluator. This would save on cost and latency, but in some cases can introduce edge cases.\n",
    "    \n",
    "    1. For the rest of the answers that are not obviously correct or do not have any semantic relation with the ground truth, this process moves to the next step in the hierarchical tree, which is using a panel of LLM evaluators. Responses that satisfy and exceed all quantitative metric thresholds can also be parsed through the LLM evaluator stage.\n",
    "\n",
    "- **Use a _Panel of LLM Evaluators_ approach to get subjective evaluations**: Refer to this [paper](https://arxiv.org/pdf/2404.18796). We use the following ways to evaluate the responses from the `candidate models` (models used to generate inferences)\n",
    "\n",
    "- **Majority Voting**: When a dataset provides a ground truth, FMBench uses a technique called `Majority Voting`. Here, we use PoLL, _or a panel of LLM evaluators_, from different model families to evaluate each candidate model's response based on whether it generates a `correct` or an `incorrect` answer simply based on its comparison with the ground truth. Using models from different model families as a PoLL, increases it's ability to match a human level evaluation, makes the evaluation process more streamlined, consistent across all the responses, and reduces the latency and cost of evaluating the candidate models over time. The intra model bias during the evaluation process is also eliminated since more than a single model acts as a panel evaluator. FMBench uses [the majority voting evaluation instructions](prompt_template/eval_criteria/evaluation_instructions_majority_vote.txt) that are fed into the prompt templates supplied to different judge models to evalaute responses at runtime.\n",
    "       \n",
    "***All evaluations are generated in a JSON format for further downstream analytics on the evaluation results***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all of the necessary libraries below to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if interactive mode is set to no -> pickup fmbench from Python installation path\n",
    "# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n",
    "# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n",
    "# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\n",
    "import os\n",
    "import sys\n",
    "if os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import ray\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from pathlib import Path\n",
    "from fuzzywuzzy import fuzz\n",
    "from fmbench.utils import *\n",
    "from fmbench.globals import *\n",
    "from numpy.linalg import norm\n",
    "from litellm import completion\n",
    "from typing import List, Optional, Dict\n",
    "import importlib.resources as pkg_resources\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger to get logs\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the ray service to run async calls in parallel to bedrock easily\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"CONFIG_FILE={CONFIG_FILE}\")\n",
    "config = load_main_config(CONFIG_FILE)\n",
    "logger.info(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the associated pricing config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "pricing_file_path: str = config['pricing'] \n",
    "\n",
    "# initialize the pricing config file to None\n",
    "pricing_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files('fmbench'), 'configs')\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "pricing_module = Path(config['pricing'])\n",
    "logger.info(f\"pricing config provided for inference from this model is --> {pricing_module}\")\n",
    "pricing_file_path = os.path.join(config_dir, pricing_module)\n",
    "logger.info(f\"pricing config file path is --> {pricing_file_path}\")\n",
    "\n",
    "pricing_config = load_config(pricing_file_path)\n",
    "logger.info(f\"pricing config file recorded: {json.dumps(pricing_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model evaluation information\n",
    "---\n",
    "\n",
    "The common model configuration file contains information about which evaluation strategy to use (`majority voting`), \n",
    "the ground truth column if provided by the user in the config file which is used in the experiment, which FMs on Bedrock to use as\n",
    "LLM as evaluators, the prompt templates used by each in the case of Majority voting, the quantitative metric thresholds\n",
    "for an answer to be correct, inference parameters and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represents getting the config file from the s3 bucket/https path for pricing yml information\n",
    "model_eval_fpath: str = config['model_evaluations']\n",
    "\n",
    "# initialize the pricing config file to None\n",
    "eval_config: Optional[Dict] = None\n",
    "\n",
    "# get the current config dir path\n",
    "config_dir = Path(pkg_resources.files('fmbench'), 'configs')\n",
    "logger.info(f\"Using fmbench.configs directory: {config_dir}\")\n",
    "\n",
    "eval_module = Path(config['model_evaluations'])\n",
    "logger.info(f\"eval config provided for evaluation --> {eval_module}\")\n",
    "eval_file_path = os.path.join(config_dir, eval_module)\n",
    "logger.info(f\"eval config file path is --> {eval_file_path}\")\n",
    "\n",
    "# eval_config = load_config(eval_file_path).format(method_name=config['method_name'])\n",
    "with open(eval_file_path, 'r') as file:\n",
    "    model_eval_info = file.read()\n",
    "    # load the preliminary unformatted config file to fetch the method name and plug it into\n",
    "    # the prompt template file names\n",
    "    model_eval_info_config =  yaml.safe_load(model_eval_info)\n",
    "    model_eval_formatted_content = model_eval_info.format(ground_truth=config['datasets'].get('ground_truth_col_key', None),\n",
    "                                                          method_name=model_eval_info_config['model_evaluations']['PoLL_Composition_and_Voting'].get('method', None))\n",
    "    eval_config = yaml.safe_load(model_eval_formatted_content)\n",
    "\n",
    "# view all information that will be used in the evaluation process, which includes the ground truth\n",
    "# in the dataset, the evaluation method (Majority voting) and associated information\n",
    "logger.info(f\"eval config file recorded: {json.dumps(eval_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug is True:\n",
    "    metrics_path_file: str = os.path.join(\"..\", \"..\", METADATA_DIR, METRICS_PATH_FNAME)\n",
    "else:\n",
    "    metrics_path_file: str = os.path.join(METADATA_DIR, METRICS_PATH_FNAME)\n",
    "logger.info(f\"cwd={os.getcwd()}, METADATA_DIR={METADATA_DIR}, METRICS_PATH_FNAME={METRICS_PATH_FNAME}, metrics_path_file={metrics_path_file}\")\n",
    "METRICS_DIR: str = Path(metrics_path_file).read_text().strip()\n",
    "logger.info(f\"metrics_path_file={metrics_path_file}, METRICS_DIR={METRICS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#file_path: str = os.path.join(METRICS_DIR, config[\"report\"][\"per_inference_request_file\"])\n",
    "file_path: str = 'fmbench-bedrock-llama3-stream-responses-fmbench-stack-us-west-2-role/data/metrics/yyyy=2024/mm=08/dd=02/hh=01/mm=09/per_inference_request_results.csv'\n",
    "logger.info(f\"File path containing the metrics per inference folder --> {file_path}\")\n",
    "\n",
    "# Read the file from S3\n",
    "try:\n",
    "    file_content = get_s3_object(config['aws']['bucket'], file_path)\n",
    "    # Use pandas to read the CSV content\n",
    "    df_per_inference = pd.read_csv(io.StringIO(file_content))\n",
    "    logger.info(f\"{file_path} read into dataframe of shape {df_per_inference.shape}, \"\n",
    "                f\"cols={df_per_inference.columns}\")\n",
    "    logger.info(f\"{file_path} contains results for the following endpoints={df_per_inference.endpoint_name.unique()}\")\n",
    "    logger.info(df_per_inference.head())\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading from S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Going to be using this inference file to generate evaluations on -> {df_per_inference.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between prompt token length and inference latency for different instances and concurrency levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Information on the inference file being used for evaluations: {df_per_inference.latency.describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Total number of inferences to evaluate from candidate models: {df_per_inference.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the `sentence-transformers/all-mpnet-base-v2` embeddings model to calculate the _Cosine Similarity_ scores \n",
    "---\n",
    "\n",
    "This portion of the evaluation step does as follows:\n",
    "\n",
    "1. Uses the `sentence-transformers/all-mpnet-base-v2` model from Hugging Face. This is a sentence-transformers model. It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "1. Use the embeddings model to get quantitative metrics from the inferences. This helps to get a similarity score between the ground truth answers from a dataset if any are given and the actual responses from the model received during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the quantitiative evaluation information from the config file, such as the embeddings model\n",
    "# to be used\n",
    "embeddings_model_quantitative_info: Dict = eval_config['model_evaluations']['quantitative_eval_info']\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    This function loads the sentence-transformers model based on the provided model ID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model=None\n",
    "        model_id = embeddings_model_quantitative_info['embeddings_model_id'].get('model_id', None)\n",
    "        if model_id:\n",
    "            model = SentenceTransformer(model_id)\n",
    "        else:\n",
    "            raise ValueError(\"Model ID is not provided or invalid in the configuration.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"The SentenceTransformer embeddings model could not be loaded: {e}\")\n",
    "        model=None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the embeddings model to calculate the cosine similarity scores\n",
    "model = load_model()\n",
    "logger.info(f\"Embeddings model info which will be used to calculate the cosine similarity scores for Majority Voting Eval: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between two texts. In this case, \n",
    "    the cosine similarity is the comparison between the ground truth in the given dataset\n",
    "    and the candidate model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cosine_similarity_score: float = None\n",
    "        # returns the embedding for a given text using the sentence-transformers model.\n",
    "        A = model.encode([text1])[0]\n",
    "        B = model.encode([text2])[0]\n",
    "        cosine_similarity_score = dot(A, B) / (norm(A) * norm(B))\n",
    "        logger.info(f\"Calculating the cosine similarity score, current score: {cosine_similarity_score}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cosine similarity was not calculated at this iteration: {e}\")\n",
    "        cosine_similarity_score=None\n",
    "    return cosine_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the method that is being used to evaluate the content (which is either Majority voting)\n",
    "model_eval_subjective_info: List[Dict] = eval_config['model_evaluations']['subjective_eval_info']\n",
    "method_name: str = eval_config['model_evaluations']['PoLL_Composition_and_Voting'].get('method', None)\n",
    "logger.info(f\"The evaluation method FMBench is going to use to evaluate different model responses: {method_name}\")\n",
    "logger.info(f\"judge panel being used to evaluate model responses: {model_eval_subjective_info.get('judge_panel_list', None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the quantitative metrics if evaluation is set to Majority voting\n",
    "logger.info(f\"Valid ground truth column found in the inference file: {eval_config['model_evaluations'].get('ground_truth_col')}, calculating cosine similarity scores\")\n",
    "logger.info(f\"~Creating embeddings and calculating cosine similarity scores for of all candidate model responses now. This might take a 1-2 minutes~\")\n",
    "ground_truth_col_name: Optional[str] = config['datasets'].get('ground_truth_col_key', None)\n",
    "\n",
    "# Check for ground truth column and raise an exception if not found\n",
    "if ground_truth_col_name is None:\n",
    "    raise ValueError(f\"Expected a valid ground truth column name in the config file information, got {ground_truth_col_name}. Cannot continue.\")\n",
    "\n",
    "# If we reach this point, we know the ground truth column exists\n",
    "df_per_inference['cosine_similarity_score'] = df_per_inference.apply(\n",
    "    lambda row: calculate_cosine_similarity(row['completion'], row['ground_truth']), axis=1\n",
    ")\n",
    "\n",
    "logger.info(f\"Calculated the cosine similarity score: {df_per_inference.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Evaluations: Hierarchical Flow\n",
    "--- \n",
    "\n",
    "1. Check for the lexical match/similarity between the ground truth and the answer using three main quantitative metrics: _Cosine similarity score, Levenshtein similarity, and Token set ratio_. \n",
    "\n",
    "1. If the thresholds of any of these or the overall quantitative metric threshold are passed (specified in the [model_eval_all_info](configs/model_eval_all_info.yml) config file), users can decide not to parse the obviously correct responses through the LLM as an evaluator process. \n",
    "\n",
    "1. For simple datasets that contain a direct answer to a question, quantitative metrics can be used to determine the correctness of an answer. However, in edge cases where quantitative metrics cannot be relied on, for example if an answer needs to be evaluated on the relationship between two people, rather than the similarity of words, then such responses can be parsed through the LLM evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Evaluation Part 1: Lexical Match & Similarity Score Evaluation Filter\n",
    "---\n",
    "\n",
    "Before having the Panel of LLM Evaluators evaluate each candidate model's response, we pass those responses through a quantitative eval step. In this step we use a threshold for a `Lexical match`, `Cosine Similarity`, and `Levenshtein Similarity` scores to define whether that answer is correct without necessarily having an LLM evaluate it. The thresholds for correctness for each quantitative metric is defined in the `model_all_eval_info.yml` config file. \n",
    "\n",
    "The reason to do this is to make the evaluation process more like a hierarchy of checks, to make sure each and every candidate model response is evaluated appropriately. Additionally, if the user decides to use the quantitative metrics as a decision point to define whether an answer is correct or incorrect without having to pass it through an LLM for evaluation can lead up to cost and latency optimization. This is specific to the `Ground Truth based approach`. \n",
    "\n",
    "For the lexical match, we use the `fuzzy` match algorithm `token_set_ratio` library to determine what percent of the two texts are similar.\n",
    "\n",
    "**Note**: `Token_set_ratio` algorithm tokenizes both input strings, removes duplicate tokens, and calculates the similarity score based on the intersection and union of the token sets. It captures the essence of the strings’ content rather than their specific order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_token_set_ratio(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the partial token match or fuzz ratio between two strings.\n",
    "    If the fuzz ratio exceeds the threshold and the cosine similarity matches or exceeds the threshold, \n",
    "    then the answer is correct and it is not evaluated using a judge. If it is not, then it\n",
    "    is parsed through the PoLL process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        token_set_ratio: float = None\n",
    "        if text1 and text2:\n",
    "            token_set_ratio = fuzz.token_set_ratio(text1, text2) / 100.0\n",
    "        else:\n",
    "            token_set_ratio=None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in calculating token set ratio: {e}\")\n",
    "        token_set_ratio=None\n",
    "    return token_set_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein distance algorithm\n",
    "---\n",
    "In information theory, linguistics, and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. The Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def levenshtein_distance(s: str, t: str):\n",
    "    \"\"\"\n",
    "    Here, we use Dynamic Programming (DP) to compute the levenshtein distance\n",
    "    between two strings\n",
    "    \"\"\"\n",
    "    # Initialize lengths of both strings\n",
    "    m, n = len(s), len(t)\n",
    "\n",
    "    # Ensure s is the longer string\n",
    "    if m < n:\n",
    "        s, t = t, s\n",
    "        m, n = n, m\n",
    "\n",
    "    # Initialize the distance matrix with dimensions (m+1) x (n+1)\n",
    "    d = [list(range(n + 1))] + [[i] + [0] * n for i in range(1, m + 1)]\n",
    "\n",
    "    # Populate the matrix\n",
    "    for j in range(1, n + 1):\n",
    "        for i in range(1, m + 1):\n",
    "            # If characters match, no cost is added\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                # Otherwise, take the minimum cost from insert, delete, or replace operations\n",
    "                d[i][j] = min(d[i - 1][j], d[i][j - 1], d[i - 1][j - 1]) + 1\n",
    "    # Return the computed Levenshtein distance (bottom-right cell of the matrix)\n",
    "    return d[m][n]\n",
    "\n",
    "\n",
    "def calculate_levenshtein_distance(input_string: str, reference_string: str) -> float:\n",
    "    \"\"\"\n",
    "    In this function, we calculate the levenshtein distance between the input string (candidate model response) and \n",
    "    the reference string (which can be the ground truth or the context provided to answer the question).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        levenshtein_similarity: Optional[float]=None\n",
    "        distance = levenshtein_distance(input_string, reference_string)\n",
    "        max_length = max(len(input_string), len(reference_string))\n",
    "        levenshtein_similarity = 1 - (distance / max_length)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not compute the levenshtein similarity score: {e}\")\n",
    "        levenshtein_similarity=None\n",
    "    return levenshtein_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # These are examples from the LongBench dataset for testing purposes\n",
    "# candidate_model_response: str = \"Both Sinofranchetia and Stauntonia are from the Lardizabalaceae family. This information is mentioned in the passages for both genera.\"\n",
    "# ground_truth: str = \"a genus of flowering plant in the Lardizabalaceae family\"\n",
    "# ratio = calculate_levenshtein_distance(candidate_model_response, ground_truth)\n",
    "# print(f\"ratio calculated: {ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the token set ratio for each row and add it as a new column\n",
    "# In this case, the ground truth is used as context to calculate the levenshtein distance\n",
    "# and the token set ratio if the ground truth is not provided\n",
    "\n",
    "# calculate the quantitative metrics if evaluation is set to Majority voting\n",
    "logger.info(f\"ground truth column is found: {eval_config['model_evaluations'].get('ground_truth_col')}, calculating token set ratio and levenshtein distance\")\n",
    "df_per_inference = df_per_inference.assign(\n",
    "    token_set_ratio_value=lambda df: df.apply(lambda row: calculate_token_set_ratio(row['completion'], row['ground_truth']), axis=1),\n",
    "    levenshtein_distance=lambda df: df.apply(lambda row: calculate_levenshtein_distance(row['completion'], row['ground_truth']), axis=1)\n",
    ")\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the all_metrics path to send the evaluation metrics to\n",
    "all_metrics_fpath: str = os.path.join(METRICS_DIR, config[\"report\"][\"all_metrics_file\"])\n",
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_cosine_similarity_scores_csv = csv_buffer.getvalue()\n",
    "inference_cosine_similarity_scores_s3_path = os.path.join(METRICS_DIR, PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(df_per_inference_with_cosine_similarity_scores_csv, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, PER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES)\n",
    "logger.info(f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{inference_cosine_similarity_scores_s3_path}\")\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Evaluation Part 2: Use _Panel of LLM Evaluators_ to get Subjective Evaluations on various evaluation criteria\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we run evaluations on all candidate models using a panel of LLM evaluators. We use a main evaluation method: `Majority Voting`. To eliminate intra-model bias, we address this by scoring answer correctness based not on a single judge, but instead on a panel composed of multiple evaluator models.\n",
    "\n",
    "1. **Majority Voting**: We use the PoLL to evaluate candidate model responses by checking its correctness compared to a provided ground truth answer in the dataset. We prompt each PoLL to evaluate and give the response in a JSON structure, giving a verdict on whether the response is correct or incorrect based on its comparison with the ground truth, and an explanation as to why that is. With all verdicts and responses in JSON, we can perform downstream tasks such as:\n",
    "\n",
    "    1. Calculate the overall accuracy of each model using the correct versus the (correct + incorrect) responses\n",
    "    \n",
    "    1. Calculate the `error rate` or frequency or incorrect responses\n",
    "    \n",
    "    1. Categorize the errors based on the explanations provided by the evaluators. Common categories might include misunderstanding the question, incomplete answers, factual inaccuracies\n",
    "    \n",
    "    1. Summary of overall correct/incorrect, and the best model based on the PoLL. Rank the models on Correctness versus Incorrectness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the qualitative/subjective evaluation information from the config file to evaluate answers from different\n",
    "# endpoints on various criteria\n",
    "model_eval_subjective_info: Dict = eval_config['model_evaluations']['subjective_eval_info']\n",
    "eval_criteria_list = model_eval_subjective_info.get('eval_criteria', None)\n",
    "logger.info(f\"available llm as a judge evaluation information to use: {json.dumps(model_eval_subjective_info, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the inference parameters that the LLM judge panel will use while evaluating model candidate responses\n",
    "INFERENCE_PARAMETERS_LLM_PANEL: Dict = eval_config['model_evaluations']['subjective_eval_info'].get('inference_parameters', None)\n",
    "logger.info(f\"Inference parameters that LLM evaluators will use: {INFERENCE_PARAMETERS_LLM_PANEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_llm_evaluation(model_id: str,\n",
    "                        prompt: str):\n",
    "    \"\"\"\n",
    "    Get inference using LiteLLM. This function is called by each evaluator on the panel of \n",
    "    llm evaluators to get a response on a given prompt. This is in the case of where there is \n",
    "    Majority voting enabled\n",
    "    \"\"\"\n",
    "    # represents the service name\n",
    "    logger.info(f\"get_inference, model_id={model_id}\")\n",
    "    service_name: str = \"bedrock\"\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name \n",
    "    # initialize the response dict\n",
    "    ret = dict(exception=None,\n",
    "               prompt=prompt,\n",
    "               completion=None,\n",
    "               completion_token_count=None,\n",
    "               prompt_token_count=None,\n",
    "               model_id=model_id)\n",
    "    body = ret['prompt']\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region\n",
    "    try:\n",
    "        # Represents calling the litellm completion/messaging api utilizing the completion/embeddings API\n",
    "        print(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(model=bedrock_model,\n",
    "                              messages=[{\"content\": body,\"role\": \"user\"}],\n",
    "                              temperature=INFERENCE_PARAMETERS_LLM_PANEL.get('temperature', 0.1),\n",
    "                              max_tokens=INFERENCE_PARAMETERS_LLM_PANEL.get('max_tokens', 100),\n",
    "                              caching=INFERENCE_PARAMETERS_LLM_PANEL.get('caching', False))\n",
    "        print(f\"response: {response}\")\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens        \n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret['exception'] = e\n",
    "    logger.info(f\"completion: {ret['completion']}\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_filename(s):\n",
    "    \"\"\"\n",
    "    convert a string to another string that can be used as a filename\n",
    "    i.e. remove white space and non-word chars\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"None\"\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespace with a single dash\n",
    "    s = re.sub(r\"\\s+\", '-', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_as_json(x: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Convert a string into a dictionary. Remove any\n",
    "    stray whitespaces which could break the json parsing\n",
    "    \"\"\"\n",
    "    d: Optional[Dict] = None\n",
    "    try:\n",
    "        x = x.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        d = json.loads(x)\n",
    "    except Exception as e:\n",
    "        print(f\"parse_as_json, error parsing string as json, string={x}\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.rename(columns={'completion': 'candidate_model_response'}, inplace=True)\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the evaluation prompt payloads\n",
    "---\n",
    "\n",
    "Here, the evaluation prompt template is used by the LLM judge to evaluate the answers on different criteria.\n",
    "This prompt template function uses a set of rules, prompt template, the answer, and ground truth (if any) in the\n",
    "evaluation solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_eval_prompts(eval_template: str,\n",
    "                         answer: str, \n",
    "                         rules: str, \n",
    "                         ground_truth: Optional[str]):\n",
    "    \"\"\"\n",
    "    This function prepares the evaluation prompts by preparing the standard eval prompt template\n",
    "    with the rules of a given subjective criteria, context, answer and ground truth (if any ground truth is provided)\n",
    "    This function prepares prompt payloads for both evaluation criteria: Majority voting. In the \n",
    "    case of Majority voting, there is no subjective criteria that is inputted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        processed_eval_template: Optional[str] = None\n",
    "        processed_eval_template = eval_template.format(\n",
    "            rules=rules,\n",
    "            answer=answer,\n",
    "            ground_truth=ground_truth)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encountered while generating the evaluation prompt template: {e}\")\n",
    "        processed_eval_template=None\n",
    "    return processed_eval_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_dir(dir_path: str):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*\"))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "# create the metrics directory that stores all of the json files containing evaluations from all Panel of LLM evaluators\n",
    "METRICS_PER_POLL_EVAL_DIR: str = os.path.join(METRICS_DIR, METRICS_PER_POLL_EVAL_DIR_NAME)\n",
    "_ = list(map(clear_dir, [METRICS_PER_POLL_EVAL_DIR]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_panel_of_llm_evals(i: int, total: int, row: Dict,  model_id: str, eval_method_name: str, uuid: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Runs the evaluation for one row \n",
    "    The eval prompt is already available in the row dictionary\n",
    "    and we simply want to run the inference against the judge model.\n",
    "    The results are returned in a new dictionary that contains the model \n",
    "    response and some fields from the original dictionary\n",
    "    \"\"\"\n",
    "    try: \n",
    "        # save all the responses from the model in a dictionary\n",
    "        resp: Optional[Dict]=None\n",
    "        logger.info(f\"run_eval, row {i}/{total}, judge_model_id={model_id}, candidate model={row['endpoint_name']}\")\n",
    "        # create the payload for model inference\n",
    "        prompt = row[f'{model_id}_{method_name}_eval_prompt']\n",
    "        # generate the evaluation on the data using the model judge\n",
    "        resp = get_llm_evaluation(model_id, prompt)\n",
    "        # assign the completion from the candidate model to the `candidate_model_response`, \n",
    "        # and the actual evaluation will be contained in a field called `completion`\n",
    "        resp['candidate_model_response'] = row['candidate_model_response']\n",
    "        logger.info(f\"Panel of LLM evaluator {model_id} completion: {resp['completion']}\")\n",
    "        resp['candidate_model'] = row['endpoint_name']\n",
    "        resp['payload_file'] = row['payload_file']\n",
    "        resp['cosine_similarity_score'] = row['cosine_similarity_score']\n",
    "        resp['levenshtein_distance'] = row['levenshtein_distance']\n",
    "        resp['token_set_ratio_value'] = row['token_set_ratio_value']\n",
    "        # if there is a ground truth (in case of Majority voting) or\n",
    "        # criteria name (in case of average pooline), include those in the json response\n",
    "        resp['ground_truth'] = row['ground_truth']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error encountered while running evaluation: {e}\")\n",
    "        resp=None\n",
    "    return resp\n",
    "\n",
    "# we use Ray to parallize\n",
    "@ray.remote\n",
    "def async_run_eval(i: int, total: int, row: Dict, model_id: str, eval_method_name: str, uuid: str) -> Dict:\n",
    "    print(f\"async_run_eval, i={i}, total={total}, judge_model_info={model_id}, eval_method: {eval_method_name}, uuid: {uuid}\")\n",
    "    return run_panel_of_llm_evals(i, total, row, model_id, eval_method_name, uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "df_per_inference_list = json.loads(df_per_inference.to_json(orient='records'))\n",
    "logger.info(f\"Total number of candidate models going to be evaluated: {len(df_per_inference_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare evaluation prompt templates\n",
    "---\n",
    "\n",
    "This portion of the step prepares the evaluation prompt templates that are used in the evaluation process of using `Majority Voting` using the PoLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_eval_subjective_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Number of judges being used for this model evaluation: {len(model_eval_subjective_info.get('judge_panel_list', None))}\")\n",
    "logger.info(f\"Inference Parameters that are going to be used by the judge panels while evaluating candidate models: {model_eval_subjective_info.get('inference_parameters', None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare prompt payloads\n",
    "---\n",
    "\n",
    "In this portion of the step, FMBench iterates through each of the row containing the model response and prepares the corresponding prompt payloads. In this step, the prompt template for a given evaluation method is used. For Majority voting, a standard prompt template is used with evaluation instructions and candidate model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming fmbench is a valid Python package and scripts is a subdirectory within it\n",
    "model_eval_dir: Optional[str] = eval_config['model_evaluations']['model_eval_dir']\n",
    "eval_prompts_dir: str = Path(pkg_resources.files('fmbench'), \n",
    "                             f\"{config['s3_read_data']['prompt_template_dir']}/{model_eval_dir.get('eval_prompts_dir', None)}\")\n",
    "\n",
    "try:\n",
    "    # Iterate through each LLM as a judge and each evaluation criterion\n",
    "    for llm_info in model_eval_subjective_info.get('judge_panel_list', []):\n",
    "        model_id: str = llm_info['model_id']\n",
    "        method_name: str = eval_config['model_evaluations']['PoLL_Composition_and_Voting'].get(\"method\", None)\n",
    "        eval_prompt_template_fname: str = f\"{llm_info.get('eval_prompt_template_name', None)}.txt\"\n",
    "\n",
    "        # Use the evaluation prompt template path to read in the standard prompt template that\n",
    "        # is used in the creation of prompt payloads\n",
    "        eval_prompt_template_dir = llm_info.get('eval_prompt_template_dir', None)\n",
    "        eval_prompt_template_path = os.path.join(eval_prompts_dir, eval_prompt_template_dir, eval_prompt_template_fname)\n",
    "        logger.info(f\"evaluation prompt template file path being used for {model_id}: {eval_prompt_template_path}\")\n",
    "        logger.info(f\"evaluation prompt template file name: {eval_prompt_template_fname}\")\n",
    "        eval_prompt_template = Path(eval_prompt_template_path).read_text()\n",
    "        logger.info(f\"Evaluation prompt template being used: {eval_prompt_template}\")\n",
    "\n",
    "        # There is a standard instructions file for both Majority voting on how to evaluate the \n",
    "        # model responses (whether it should be a binary decision or rating on a scale of 1-5)\n",
    "        eval_instructions_fname = next((rule for rule in model_eval_dir.get('eval_instructions_files', None) if method_name in rule), None)\n",
    "        rules = Path(os.path.join(eval_prompts_dir, eval_instructions_fname)).read_text()\n",
    "        logger.info(f\"rules: {rules}\")\n",
    "        column_name = f\"{model_id}_{method_name}_eval_prompt\"\n",
    "        df_per_inference[column_name] = df_per_inference.apply(\n",
    "            lambda r: prepare_eval_prompts(\n",
    "                eval_prompt_template,\n",
    "                r['candidate_model_response'],\n",
    "                rules,\n",
    "                r['ground_truth']\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error occurred in the creation of prompt payloads: {e}\")\n",
    "    df_per_inference=None\n",
    "\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_buffer = io.StringIO()\n",
    "df_per_inference.to_csv(csv_buffer, index=False)\n",
    "df_per_inference_with_eval_prompt_payloads = csv_buffer.getvalue()\n",
    "eval_prompt_payloads_for_inference = os.path.join(METRICS_DIR, PROCESSED_EVAL_PROMPT_PAYLOADS)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(df_per_inference_with_eval_prompt_payloads, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, PROCESSED_EVAL_PROMPT_PAYLOADS)\n",
    "logger.info(f\"Per inference cosine similarity scores saved to s3://{BUCKET_NAME}/{eval_prompt_payloads_for_inference}\")\n",
    "df_per_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_per_inference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the dataframe into a list of dicts as that is easy to parallize via Ray\n",
    "eval_records_list = json.loads(df_per_inference.to_json(orient='records'))\n",
    "logger.info(f\"Total number evaluations to be done: {len(eval_records_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the hierarchy of Model Evaluations\n",
    "---\n",
    "\n",
    "In this portion of the step, FMBench performs the following actions:\n",
    "\n",
    "1. For `Majority Voting` - We suppose that a ground truth already exists in the dataset. We first calculate quantitative metrics. If the desired quantitative threshold for either cosine similarity, levenshtein distance or token set ratio is exceeded, we mark the candidate model response as correct if the user enables the `use_quantitative_metrics` parameter in the common model eval config file. If none of the thresholds are passed, then we check for the overall metric and whether that satisfies the average quantitative threshold. If it does, customers can decide to filter out those responses that are passing the threshold and assume those are correct. If not, all the questions can be supplied to the panel of LLMs for the next set of evaluations.\n",
    "\n",
    "1. We use the LLM panel of judges (in this case 3 judges), to give a verdict on whether the `answer` from the candidate models during inference is `correct` or `incorrect`. The panel of LLM judges also gives an explanation as to why it evaluated a candidate model response as correct or incorrect.\n",
    "\n",
    "1. Each model response is given in a JSON structure which is further used for downstream analytics, to decide the comparision of evaluation results between different model candidates and more.\n",
    "\n",
    "***This step takes a couple of minutes to complete based on the size of the dataset and the judge models. Model completion time depends on the PoLL models being used. `Llama3-70b`, `Cohere command-r-v1` and `claude 3 Sonnet` were used for this example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the llm as a judge panel list\n",
    "judge_panel_list: List[Dict] = model_eval_subjective_info.get('judge_panel_list', None)\n",
    "logger.info(f\"The judge panel list contains {len(judge_panel_list)} judges. Their information: {judge_panel_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"~Panel of LLM evaluators are going to start evaluating responses. This might take a couple of minutes depending on the size of the dataset and candidate model responses~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_quantitative_eval_enabled: bool = eval_config['model_evaluations']['PoLL_Composition_and_Voting'].get('use_quantitative_metrics', False)\n",
    "logger.info(f\"Are quantitative metrics going to be used to make a final eval decision: {is_quantitative_eval_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the evaluation process\n",
    "---\n",
    "\n",
    "This process loops through the evaluation prompt payloads that are prepared. For Majority voting, a JSON containing 2 elements is generated: \"verdict\" of whether the given answer is correct or incorrect and an \"explanation\". \n",
    "\n",
    "Responses from either evaluation processes are sent for further downstream processes to determine the most accurate\n",
    "and subjectively correct model based on domain specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n: int = model_eval_subjective_info.get('run_parallel_inference_count', 5)\n",
    "list_of_lists = [eval_records_list[i * n:(i + 1) * n] for i in range((len(eval_records_list) + n - 1) // n)]\n",
    "resp_list = []\n",
    "erroneous_count: int = 0\n",
    "st: float = time.perf_counter()\n",
    "\n",
    "# Iterate over the judge panel and sublists\n",
    "for judge_panelist_info in judge_panel_list:\n",
    "    logger.info(f\"============Running inference for judge panelist {judge_panelist_info['model_id']} for {method_name} ============\")\n",
    "    for idx, sublist in enumerate(list_of_lists):\n",
    "        model_id: str = judge_panelist_info['model_id']\n",
    "        logger.info(f\"Getting inference for list {idx + 1}/{len(list_of_lists)}, size of list={len(sublist)}\")\n",
    "        try:\n",
    "            resp_list.extend(ray.get([async_run_eval.remote(i + 1, len(sublist), record, model_id, method_name, record['uuid'])\n",
    "                               for i, record in enumerate(sublist)]))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing list {idx + 1}/{len(list_of_lists)}: {e}\")\n",
    "            erroneous_count += 1\n",
    "    # Sleep for two seconds before moving on to the next model\n",
    "    logger.info(f\"~Sleeping for one second before the next Panel of LLM evaluates the responses~\")\n",
    "    time.sleep(1)\n",
    "\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"Total elapsed time for inference: {elapsed_time:.2f} seconds\")\n",
    "logger.info(f\"Total erroneous lists: {erroneous_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send all Panel of LLM evaluator responses to S3 as `JSON` files\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect all of the panel of LLM evals and send them all as JSON files to S3\n",
    "if resp_list:\n",
    "    save_s3_list = []\n",
    "    try:\n",
    "        for resp in resp_list:\n",
    "            llm_eval_response = json.dumps(resp, indent=2)\n",
    "            candidate_model_id = resp.get('candidate_model', None)\n",
    "            # Extract a few words from the poll eval response to append to the file name\n",
    "            response_excerpt = \" \".join(resp.get('candidate_model_response', \"\").split()[:5])\n",
    "            sanitized_response_excerpt = \"\".join([c if c.isalnum() else \"_\" for c in response_excerpt])\n",
    "            llm_eval_json_fname = f\"{candidate_model_id}_{time.time()}_{sanitized_response_excerpt}.json\"\n",
    "            response_s3_path = os.path.join(METRICS_PER_POLL_EVAL_DIR, llm_eval_json_fname)\n",
    "            logger.info(f\"Sending model eval result files to s3 path prefix: {response_s3_path}\")\n",
    "            save_s3_list.append((llm_eval_response,\n",
    "                                config['aws']['bucket'],\n",
    "                                \"\",\n",
    "                                METRICS_PER_POLL_EVAL_DIR,\n",
    "                                llm_eval_json_fname))\n",
    "\n",
    "        # Split the save_s3_list into smaller batches to get\n",
    "        # rid of the cannot write to s3 bucket - request rate was hitting maximum threshold\n",
    "        batch_size: int = 50\n",
    "        delay: float = 1 \n",
    "        for i in range(0, len(save_s3_list), batch_size):\n",
    "            batch = save_s3_list[i:i + batch_size]\n",
    "            # write a batch of evaluation result files to s3\n",
    "            write_multiple_to_s3(batch)\n",
    "            time.sleep(delay)  # Delay between batches\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing or writing to S3: {e}\")\n",
    "else:\n",
    "    logger.info(\"No responses to write to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save All Results: Perform downstream analytical tasks on each PoLL evaluation result\n",
    "---\n",
    "\n",
    "In this portion of the evaluation step:\n",
    "\n",
    "1. We compile all metrics gathered from the Majority Voting experiment, and send them as `CSV`, `txt` files to s3.\n",
    "\n",
    "1. These metrics include: Quantitative metrics and binary decision scores (for Majority Voting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the results list into a dataframe for easy analytics\n",
    "df_eval_results = pd.DataFrame(resp_list)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how='all')\n",
    "# the exception, judge model id, prompt token count, will be NaN for the verdicts decided\n",
    "# using the lexical match and not moved forward to the panel of LLM evaluators\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parse out the completion from LLM as a judge and column bind\n",
    "# the fields of the dictionary to the original results dataframe\n",
    "df_eval_results_only = df_eval_results['completion'].apply(parse_as_json).apply(pd.Series)\n",
    "df_eval_results_only.dropna(axis=1, how='all')\n",
    "df_eval_results = pd.concat([df_eval_results, df_eval_results_only], axis=1)\n",
    "df_eval_results.rename(columns={'model_id': 'judge_model_id'}, inplace=True)\n",
    "logger.info(f\"df_eval_results shape={df_eval_results.shape}\")\n",
    "df_eval_results.dropna(axis=1, how='all')\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the correctness of LLM Evaluators using quantitative metrics\n",
    "---\n",
    "\n",
    "In this portion of the evaluation step, we perform the following steps:\n",
    "\n",
    "1. Evaluate whether the LLM evaluators sent in the correct evaluations using another layer of checks with _Cosine Similarity Score_. \n",
    "\n",
    "1. If the verdicts decided by the LLM evaluators (`correct` or `incorrect`) do not meet the respective cosine similarity thresholds, then they are sent into another file for further analysis for human or another LLM evaluation loop. \n",
    "\n",
    "There are two possible cases for this evaluation: \n",
    "\n",
    "1. **Incorrect Verdicts**: If the verdict from the judge model is incorrect, then check if the cosine similarity of that\n",
    "    incorrectly identified verdict is less than the `incorrect_verdict_cosine_similarity_threshold`. If so, then it is \n",
    "    finally sent in as is into the dataframe. If the LLM evaluator defines a verdict as incorrect but if it has a higher cosine \n",
    "    similarity than the incorrect cosine similarity threshold, then it is marked for \"needing further evaluation using a human\" or\n",
    "    another LLM evalution.\n",
    "\n",
    "2. **Correct Verdicts**: If the verdict from the judge model is correct and if it exceeds the correctness cosine similarity threshold, \n",
    "    then the model is evaluated as correct and sent in for further downstream analytics. For the correct verdicts identified by the judge models\n",
    "    that do not meet the correctness cosine similarity threshold, are defined as \"needed further human/LLM evaluation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantitative_verdict_cosine_similarity_decision(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Given an LLM evaluator response, this function checks for whether a verdict provided by an LLM evaluator \n",
    "    is correctly evaluated using a cosine similarity metric threshold for correct and incorrect verdicts. These\n",
    "    are the two cases that this function handles for each evaluation done using LLM as evaluators:\n",
    "\n",
    "    1. Incorrect Verdicts: If the verdict from the judge model is incorrect, then check if the cosine similarity of that\n",
    "    incorrectly identified verdict is less than the `incorrect_verdict_cosine_similarity_threshold`. If so, then it is \n",
    "    finally sent in as is into the dataframe. If the LLM evaluator defines a verdict as incorrect but if it has a higher cosine \n",
    "    similarity than the incorrect cosine similarity threshold, then it is marked for \"needing further evaluation using a human\" or\n",
    "    another LLM evalution.\n",
    "\n",
    "    2. Correct Verdicts: If the verdict from the judge model is correct and if it exceeds the correctness cosine similarity threshold, \n",
    "    then the model is evaluated as correct and sent in for further downstream analytics. For the correct verdicts identified by the judge models\n",
    "    that do not meet the correctness cosine similarity threshold, are defined as \"needed further human/LLM evaluation\".\n",
    "\n",
    "    This function is used if the evaluation method being used is Majority voting, specifically in the case\n",
    "    of when ground truth is provided.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This is a boolean value that is returned defining whether a given verdict is valid based on \n",
    "        # the comparison of its respective cosine similarity score and cosine similarity threshold for correctness/incorrectness\n",
    "        is_eval_done_correctly: Optional[bool] = None\n",
    "        correct_cosine_similarity_threshold: Optional[float] = None\n",
    "        incorrect_cosine_similarity_threshold: Optional[float] = None\n",
    "\n",
    "        # Check if the evaluation method is Majority voting and if the customer has enabled\n",
    "        # evaluation decisions to also be made by quantitative metric thresholds\n",
    "        if is_quantitative_eval_enabled:\n",
    "            # Retrieve the information that is going to be used to check for whether a verdict is \n",
    "            # incorrectly identified as correct or incorrect\n",
    "            judge_model_id: str = row['judge_model_id']\n",
    "            verdict: str = row['verdict']\n",
    "            explanation: str = row['explanation']\n",
    "            cosine_similarity_score: float = row['cosine_similarity_score']\n",
    "\n",
    "            # Get the correctness and incorrectness cosine similarity threshold scores\n",
    "            correct_cosine_similarity_threshold = eval_config['model_evaluations']['quantitative_eval_info'].get('correct_verdict_cosine_similarity_threshold', None)\n",
    "            incorrect_cosine_similarity_threshold = eval_config['model_evaluations']['quantitative_eval_info'].get('incorrect_verdict_cosine_similarity_threshold', None)\n",
    "\n",
    "            # If the verdict is correct and is greater than or equal to the correct cosine similarity threshold, then \n",
    "            # the verdict is correct. If not, the verdict is identified to need further evaluation\n",
    "            if verdict == 'correct':\n",
    "                if cosine_similarity_score >= correct_cosine_similarity_threshold:\n",
    "                    row['explanation'] = f\"Judge model explanation: {explanation}. Cosine similarity is {cosine_similarity_score}, which does meets the threshold of {correct_cosine_similarity_threshold}.\"\n",
    "                    is_eval_done_correctly = True\n",
    "                else:\n",
    "                    row['verdict'] = \"needs further human/LLM evaluation\"\n",
    "                    row['explanation'] = f\"Judge model explanation: {explanation}. Cosine similarity is {cosine_similarity_score}, which does not meet the threshold of {correct_cosine_similarity_threshold}. Evaluate it further to determine the correct answer.\"\n",
    "                    is_eval_done_correctly = False\n",
    "\n",
    "            # If the verdict is incorrect and is less than or equal to the incorrect cosine similarity threshold, then \n",
    "            # the verdict is correctly identified as incorrect. If not, the verdict is identified to need further evaluation\n",
    "            elif verdict == 'incorrect':\n",
    "                if cosine_similarity_score <= incorrect_cosine_similarity_threshold:\n",
    "                    row['explanation'] = f\"Judge model explanation: {explanation}. Cosine similarity is {cosine_similarity_score}, which does meets the threshold of {correct_cosine_similarity_threshold}.\"\n",
    "                    is_eval_done_correctly = True\n",
    "                else:\n",
    "                    row['verdict'] = \"needs further human/LLM evaluation\"\n",
    "                    row['explanation'] = f\"Judge model explanation: {explanation}. Cosine similarity is {cosine_similarity_score}, which does not meet the threshold of {incorrect_cosine_similarity_threshold}. Evaluate it further to determine the correct answer.\"\n",
    "                    is_eval_done_correctly = False\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in quantitative_verdict_cosine_similarity_decision: {str(e)}\")\n",
    "        is_eval_done_correctly = None\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the layer of another evaluation filter on the dataframe containing all LLM as evaluator results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if df_eval_results is not None:\n",
    "    df_eval_results = df_eval_results.apply(lambda r: quantitative_verdict_cosine_similarity_decision(r), axis=1)\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_eval_results[df_eval_results['verdict'] == 'needs further human/LLM evaluation'].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send the raw results as a csv file to the S3 bucket\n",
    "csv_buffer = io.StringIO()\n",
    "df_eval_results.to_csv(csv_buffer, index=False)\n",
    "eval_llm_as_a_judge_results = csv_buffer.getvalue()\n",
    "eval_results_csv_fpath = os.path.join(METRICS_DIR, MODEL_EVAL_COMPLETIONS_CSV)  # Define full S3 path\n",
    "\n",
    "# Write the CSV data to S3\n",
    "write_to_s3(eval_llm_as_a_judge_results, BUCKET_NAME, \"\", \n",
    "            METRICS_DIR, MODEL_EVAL_COMPLETIONS_CSV)\n",
    "logger.info(f\"Per PoLL model responses saved as a csv to s3://{BUCKET_NAME}/{eval_results_csv_fpath}\")\n",
    "df_eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Total number of evaluations that are done using different panel of LLM evaluators: {df_eval_results.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Voting Results: Send the incorrect and correct responses to S3 separately in `CSV` files for downstream analytics for each model judge\n",
    "---\n",
    "\n",
    "In this portion of the step, we will send the model responses as CSV, txt files to s3 for further downstream processing and report generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For Majority Voting - all responses from the panel of LLM as evaluators are sent \n",
    "# to s3 as a csv file\n",
    "try:\n",
    "    logger.info(f\"Method name is {method_name}, sending the correct and incorrect verdicts to s3\")\n",
    "    verdict_types: List[str] = ['incorrect', 'correct']\n",
    "    all_llm_eval_responses_df: Optional[pd.DataFrame] = None\n",
    "    # iterate through each of the verdict tupe and save each verdict type responses from each evaluator in different\n",
    "    # csv files. For example, a csv files containing only incorrect verdicts from all model judges, whereas another \n",
    "    # csv file containing only the correct verdicts.\n",
    "    for verdict in verdict_types:\n",
    "        df_verdicts = df_eval_results[df_eval_results['verdict'] == verdict]\n",
    "        all_llm_eval_responses_df = pd.concat([all_llm_eval_responses_df, df_verdicts], ignore_index=True)\n",
    "        if not df_verdicts.empty:\n",
    "            csv_buffer = io.StringIO()\n",
    "            df_verdicts.to_csv(csv_buffer, index=False)\n",
    "            verdict_responses = csv_buffer.getvalue()\n",
    "            verdict_file = INCORRECT_VERDICT_RESPONSES_FILE if verdict == 'incorrect' else CORRECT_VERDICT_RESPONSES_FILE\n",
    "            verdict_responses_fpath = os.path.join(METRICS_DIR, verdict_file)\n",
    "            write_to_s3(verdict_responses, BUCKET_NAME, \"\", METRICS_DIR, verdict_file)\n",
    "            logger.info(f\"{verdict.capitalize()} verdict responses sent to s3://{BUCKET_NAME}/{verdict_responses_fpath}\")\n",
    "            logger.info(f\"Number of {verdict} responses in total: {df_verdicts.shape[0]}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error encountered while writing the evaluation responses to s3: {e}\")\n",
    "    all_llm_eval_responses_df = None\n",
    "\n",
    "all_llm_eval_responses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For Majority Voting - send all incorrect and correct verdicts as txt files to s3 for readability purposes\n",
    "try:\n",
    "    logger.info(f\"Method name is {method_name}, sending the correct and incorrect verdicts to s3\")\n",
    "    verdict_types: List[str] = ['incorrect', 'correct']\n",
    "    judge_model_ids = df_eval_results['judge_model_id'].unique()\n",
    "    # save each judge model's correct and incorrect verdict files as txt files\n",
    "    # for downstream analytics and readability purposes\n",
    "    for judge_model_id in judge_model_ids:\n",
    "        for verdict in verdict_types:\n",
    "            df_judge_verdict = df_eval_results[(df_eval_results['verdict'] == verdict) & (df_eval_results['judge_model_id'] == judge_model_id)]\n",
    "            if not df_judge_verdict.empty:\n",
    "                txt_buffer = io.StringIO()\n",
    "                for index, row in df_judge_verdict.iterrows():\n",
    "                    txt_buffer.write(\n",
    "                        f\"candidate model: {row['candidate_model']}\\n\"\n",
    "                        f\"candidate model response: {row['candidate_model_response']}\\n\"\n",
    "                        f\"ground truth: {row['ground_truth']}\\n\"\n",
    "                        f\"verdict and explanation: {row['completion']}\\n\\n\"\n",
    "                    )\n",
    "                judge_verdict_responses = txt_buffer.getvalue()\n",
    "                verdict_file = f\"{judge_model_id}_{verdict}_verdicts_evaluation.txt\"\n",
    "                judge_verdict_responses_fpath = os.path.join(METRICS_DIR, verdict_file)\n",
    "                write_to_s3(judge_verdict_responses, BUCKET_NAME, \"\", METRICS_DIR, verdict_file)\n",
    "                logger.info(f\"{verdict.capitalize()} verdict responses for judge {judge_model_id} saved to s3://{BUCKET_NAME}/{judge_verdict_responses_fpath}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error encountered while writing the evaluation responses to s3: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the overall quantitate metrics of each model scored by the PoLL\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mean cosine similarity score, levenshtein distance and token set ratio\n",
    "try:\n",
    "    panel_summary_responses_df = df_eval_results.groupby(['judge_model_id', 'candidate_model', 'verdict']).agg(\n",
    "        count=('verdict', 'size'),\n",
    "        mean_cosine_similarity=('cosine_similarity_score', 'mean'),\n",
    "        mean_levenshtein_distance=('levenshtein_distance', 'mean'),\n",
    "        mean_token_set_ratio=('token_set_ratio_value', 'mean')\n",
    "    ).unstack(fill_value=0).stack().reset_index()\n",
    "    csv_buffer = io.StringIO()\n",
    "    panel_summary_responses_df.to_csv(csv_buffer, index=False)\n",
    "    panel_summary_responses = csv_buffer.getvalue()\n",
    "    llm_as_a_judge_per_eval_summary_fpath = os.path.join(METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES)\n",
    "    write_to_s3(panel_summary_responses, BUCKET_NAME, \"\", METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES)\n",
    "    logger.info(f\"Summary on each eval (Majority voting) for each panel judge sent to s3://{BUCKET_NAME}/{llm_as_a_judge_per_eval_summary_fpath}\")\n",
    "    logger.info(f\"View information on the accuracy metrics: {panel_summary_responses_df.head()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate the overall accuracy metrics for Majority Voting: {e}\")\n",
    "panel_summary_responses_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    panel_summary_responses_df = df_eval_results.groupby(['judge_model_id', 'candidate_model', 'verdict']).agg(\n",
    "        count=('verdict', 'size'),\n",
    "        mean_cosine_similarity=('cosine_similarity_score', 'mean'),\n",
    "        mean_levenshtein_distance=('levenshtein_distance', 'mean'),\n",
    "        mean_token_set_ratio=('token_set_ratio_value', 'mean')\n",
    "    ).unstack(fill_value=0).stack().reset_index()\n",
    "    csv_buffer = io.StringIO()\n",
    "    panel_summary_responses_df.to_csv(csv_buffer, index=False)\n",
    "    panel_summary_responses = csv_buffer.getvalue()\n",
    "    llm_as_a_judge_per_eval_summary_fpath = os.path.join(METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES)\n",
    "\n",
    "    write_to_s3(panel_summary_responses, BUCKET_NAME, \"\", METRICS_DIR, LLM_JUDGE_PANEL_RESPONSE_SUMMARIES)\n",
    "    logger.info(f\"Summary on each eval (Majority voting) for each panel judge sent to s3://{BUCKET_NAME}/{llm_as_a_judge_per_eval_summary_fpath}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate the overall accuracy metrics for Majority Voting: {e}\")\n",
    "\n",
    "panel_summary_responses_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get the per panel judgement on each candidate model in terms of the \n",
    "    # how many responses where correct (accuracy) and how many were incorrect (error rate)\n",
    "    per_panel_judgement_result_df = panel_summary_responses_df.pivot_table(\n",
    "        index=['candidate_model', 'judge_model_id'],\n",
    "        columns='verdict',\n",
    "        values='count',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "\n",
    "    # Ensure 'correct' and 'incorrect' columns exist\n",
    "    if 'correct' not in per_panel_judgement_result_df.columns:\n",
    "        per_panel_judgement_result_df['correct'] = 0\n",
    "    if 'incorrect' not in per_panel_judgement_result_df.columns:\n",
    "        per_panel_judgement_result_df['incorrect'] = 0\n",
    "\n",
    "    # Calculate accuracy and error rate\n",
    "    per_panel_judgement_result_df = per_panel_judgement_result_df.assign(\n",
    "        accuracy=lambda df: df.apply(lambda row: 100 if row['incorrect'] == 0 else round(row['correct'] / (row['correct'] + row['incorrect']), 2) * 100, axis=1),\n",
    "        error_rate=lambda df: df.apply(lambda row: 0 if row['incorrect'] == 0 else round(row['incorrect'] / (row['correct'] + row['incorrect']), 2) * 100, axis=1)\n",
    "    )\n",
    "    per_panel_judgement_result_df.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate the overall accuracy metrics for Majority Voting: {e}\")\n",
    "\n",
    "per_panel_judgement_result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mean_cosine_similarity = df_eval_results.groupby('candidate_model')['cosine_similarity_score'].mean().reset_index().rename(columns={'cosine_similarity_score': 'mean_cosine_similarity'})\n",
    "    mean_levenshtein_distance = df_eval_results.groupby('candidate_model')['levenshtein_distance'].mean().reset_index().rename(columns={'levenshtein_distance': 'mean_levenshtein_distance'})\n",
    "    mean_token_set_ratio = df_eval_results.groupby('candidate_model')['token_set_ratio_value'].mean().reset_index().rename(columns={'token_set_ratio_value': 'mean_token_set_ratio_value'})\n",
    "\n",
    "    overall_accuracy_grouped_panel_df = per_panel_judgement_result_df.groupby('candidate_model')[['accuracy', 'error_rate']].mean().reset_index()\n",
    "    overall_accuracy_grouped_panel_df = (\n",
    "        pd.merge(mean_cosine_similarity, overall_accuracy_grouped_panel_df, on='candidate_model')\n",
    "        .merge(mean_levenshtein_distance, on='candidate_model')\n",
    "        .merge(mean_token_set_ratio, on='candidate_model')\n",
    "        .sort_values(by='accuracy', ascending=False)\n",
    "    )\n",
    "\n",
    "    # Send the accuracy metrics to S3\n",
    "    csv_buffer = io.StringIO()\n",
    "    overall_accuracy_grouped_panel_df.to_csv(csv_buffer, index=False)\n",
    "    overall_panel_result = csv_buffer.getvalue()\n",
    "    overall_panel_accuracy_metrics_fpath = os.path.join(METRICS_DIR, PER_MODEL_ACCURACY_POLL)\n",
    "\n",
    "    write_to_s3(overall_panel_result, BUCKET_NAME, \"\", METRICS_DIR, PER_MODEL_ACCURACY_POLL)\n",
    "    logger.info(f\"Overall accuracy and error rates results of each model sent to s3://{BUCKET_NAME}/{overall_panel_accuracy_metrics_fpath}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate the overall accuracy metrics for Majority Voting: {e}\")\n",
    "\n",
    "overall_accuracy_grouped_panel_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send all responses from the evaluation process to S3 as a txt file for further downstream processing and readability purposes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Write all explanations to a file and send to S3\n",
    "    explanations_txt_buffer = io.StringIO()\n",
    "    for index, row in df_eval_results.iterrows():\n",
    "        explanations_txt_buffer.write(\n",
    "            f\"candidate model: {row['candidate_model']}\\n\"\n",
    "            f\"candidate model response: {row['candidate_model_response']}\\n\"\n",
    "            f\"ground truth: {row['ground_truth']}\\n\"\n",
    "            f\"verdict and explanation: {row['completion']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    explanations_txt_file_content = explanations_txt_buffer.getvalue()\n",
    "    explanations_fpath = os.path.join(METRICS_DIR, ALL_EVALUATIONS_IN_TXT)\n",
    "    write_to_s3(explanations_txt_file_content, BUCKET_NAME, \"\", METRICS_DIR, ALL_EVALUATIONS_IN_TXT)\n",
    "    logger.info(f\"All text eval content from the llm judge panelists sent to s3://{BUCKET_NAME}/{explanations_fpath}\")\n",
    "    logger.info(f\"All of the content including the candidate model responses, ground truth, evaluation are written: {explanations_txt_file_content}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not calculate the overall accuracy metrics for Majority Voting: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_fmbench_python311",
   "language": "python",
   "name": "conda_fmbench_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
